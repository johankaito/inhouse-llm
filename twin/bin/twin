#!/usr/bin/env python3
"""
twin - Digital Twin Local LLM Wrapper
Replicates Claude Code planning experience with local LLMs
"""

import sys
import os
from pathlib import Path

# Add lib directory to path (resolve symlinks)
script_path = Path(__file__).resolve()
lib_path = script_path.parent.parent / "lib"
sys.path.insert(0, str(lib_path))

import click
from rich.console import Console
from rich.panel import Panel
from rich.markdown import Markdown

from config import ConfigLoader
from modes import ModeDetector
from agents import AgentLoader
from context import ContextManager
from session import SessionOrchestrator

console = Console()


@click.command()
@click.option('--mode', type=click.Choice(['work', 'personal']), help='Force specific mode')
@click.option('--agent', help='Start with specific agent')
@click.option('--model', default=None, help='Ollama model or alias (fast/balanced/quality/reasoning)')
@click.option('--no-context', is_flag=True, help='Ignore previous context')
@click.option('-c', '--command', 'command_prompt', default=None, help='One-shot prompt for non-interactive mode')
def main(mode, agent, model, no_context, command_prompt):
    """twin - Digital Twin Local Planning Mode"""

    try:
        # Load configuration
        config_loader = ConfigLoader()
        config = config_loader.load_all()

        # Determine model to use (CLI arg > config default)
        if model is None:
            # No CLI arg, use default from config
            twin_config = config.get('twin_config', {})
            model_alias = twin_config.get('default_model', 'fast')
        else:
            model_alias = model

        # Resolve alias to actual model name
        resolved_model = config_loader.resolve_model_alias(model_alias)

        # Validate model exists
        exists, available_models = config_loader.validate_model_exists(resolved_model)

        if not exists:
            console.print(f"[yellow]‚ö†Ô∏è  Model '{resolved_model}' not found in Ollama[/yellow]")
            console.print(f"[yellow]Available models:[/yellow]")
            for m in available_models:
                console.print(f"  - {m}")

            # Fallback to fast model
            fallback_alias = 'fast'
            fallback_model = config_loader.resolve_model_alias(fallback_alias)
            fb_exists, _ = config_loader.validate_model_exists(fallback_model)

            if fb_exists:
                console.print(f"\n[green]‚úì Falling back to '{fallback_alias}' ({fallback_model})[/green]\n")
                resolved_model = fallback_model
            else:
                console.print(f"\n[red]‚úó Fallback model '{fallback_model}' also not found[/red]")
                console.print(f"[red]Run: ollama pull {fallback_model}[/red]\n")
                sys.exit(1)

        # Use validated model
        model = resolved_model

        # Detect mode
        mode_detector = ModeDetector(config)
        detected_mode = mode or mode_detector.detect(os.getcwd())

        # Load agents
        agent_loader = AgentLoader(config)
        agents = agent_loader.load_all()

        # Select agent
        if agent:
            selected_agent = agent_loader.get_agent(agent)
        else:
            selected_agent = agent_loader.get_default_for_mode(detected_mode)

        # Load context
        context_manager = ContextManager(config)
        if not no_context:
            context = context_manager.load_context(os.getcwd())
        else:
            context = None

        # Display startup banner
        display_banner(detected_mode, selected_agent, context, model)

        # Start session
        orchestrator = SessionOrchestrator(
            config=config,
            mode=detected_mode,
            agent=selected_agent,
            context=context,
            model=model,
            agent_loader=agent_loader,
            mode_detector=mode_detector,
            context_manager=context_manager
        )

        if command_prompt:
            # Feed prompt via stdin-style path
            import io
            sys.stdin = io.StringIO(command_prompt)
            orchestrator.run()
        else:
            orchestrator.run()

    except KeyboardInterrupt:
        console.print("\n\n‚úã [yellow]Session cancelled[/yellow]")
        sys.exit(0)
    except Exception as e:
        console.print(f"\n‚ùå [red]Error: {e}[/red]")
        if os.getenv('DEBUG'):
            raise
        sys.exit(1)


def display_banner(mode, agent, context, model):
    """Display startup banner"""
    mode_emoji = "üè¢" if mode == "work" else "üè†"
    mode_color = "cyan" if mode == "work" else "green"

    banner_text = f"""
[bold]üß† Digital Twin - Local Planning Mode[/bold]

{mode_emoji} Mode: [{mode_color}]{mode.upper()}[/{mode_color}]
ü§ñ Agent: [yellow]{agent['name']}[/yellow]
üîß Model: [dim]{model}[/dim]
"""

    if context:
        session_count = len(context.get('sessions', []))
        last_session = context.get('sessions', [])[-1] if context.get('sessions') else None
        if last_session:
            banner_text += f"\nüìÇ Context: [dim]Found {session_count} previous session(s)[/dim]"
            banner_text += f"\nüìÖ Last: [dim]{last_session.get('timestamp', 'unknown')}[/dim]"
    else:
        banner_text += "\nüìÇ Context: [dim]Starting fresh[/dim]"

    banner_text += "\n\n[dim]Type your question or /help for commands[/dim]"

    console.print(Panel(banner_text, border_style=mode_color))


if __name__ == "__main__":
    main()
